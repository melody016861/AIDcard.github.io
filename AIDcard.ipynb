{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUk9wNuBiw/EZjoJ/Bf3n0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melody016861/AIDcard.github.io/blob/main/AIDcard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 智慧回收裝置\n",
        "\n",
        "## 平台功能\n",
        "\n",
        "1. 提供使用者上傳回收分類圖片\n",
        "2. 爬蟲建立細分類資料集(提供程式碼)\n",
        "3. 智慧垃圾裝置地圖\n",
        "4. 聊天機器\n",
        "5. 回收資訊和教育宣導\n",
        "6. 回收記錄(目前只能以手動紀錄)\n",
        "7. 實時資訊更新獎勵機制\n",
        "8. 社群互動和分享\n",
        "9. 用戶反饋和改善\n",
        "10. 小遊戲"
      ],
      "metadata": {
        "id": "vCtJ5NdoXZI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 提供使用者上傳回收分類圖片\n",
        "https://drive.google.com/drive/folders/1ddIq1rjWLzNe3BTafZjxqnx3jMDucKUf?usp=sharing"
      ],
      "metadata": {
        "id": "Q0s0JnqLXwIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 爬蟲建立細分類資料集"
      ],
      "metadata": {
        "id": "O4KBZ2emaNna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xV0jAkdQcFZ",
        "outputId": "13ffdfb9-4fa8-4134-ba9a-50acdddd303c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_recycling_categories(url):\n",
        "    # 發送 GET 請求以獲取網頁內容\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # 檢查請求是否成功\n",
        "    if response.status_code == 200:\n",
        "        # 使用 BeautifulSoup 解析網頁內容\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # 找到包含回收種類信息的 HTML 元素\n",
        "        categories_container = soup.find('div', class_='recycling-categories')\n",
        "\n",
        "        # 初始化回收種類列表\n",
        "        recycling_categories = []\n",
        "\n",
        "        # 提取回收種類信息\n",
        "        if categories_container:\n",
        "            categories = categories_container.find_all('li')\n",
        "            for category in categories:\n",
        "                recycling_categories.append(category.text.strip())\n",
        "\n",
        "        return recycling_categories\n",
        "    else:\n",
        "        # 請求失敗時返回空列表\n",
        "        print(\"無法獲取資料。狀態碼:\", response.status_code)\n",
        "        return []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 指定目標網站的 URL\n",
        "    target_url = 'https://example.com/recycling-categories'\n",
        "\n",
        "    # 獲取回收種類信息\n",
        "    categories = get_recycling_categories(target_url)\n",
        "\n",
        "    # 打印獲取到的回收種類信息\n",
        "    if categories:\n",
        "        print(\"回收種類:\")\n",
        "        for category in categories:\n",
        "            print(category)\n",
        "    else:\n",
        "        print(\"未找到回收種類。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyNLDvxWowEo",
        "outputId": "f347c2f4-58b6-4684-d5c4-7c0ce829cb30"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "無法獲取資料。狀態碼: 500\n",
            "未找到回收種類。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "\n",
        "def scrape_images_for_category(category_name, num_images, save_dir):\n",
        "    # 定義要爬取的類別的網址\n",
        "    base_url = \"https://www.example.com\"  # 替換為實際的網站URL\n",
        "    search_query = urllib.parse.quote(category_name)\n",
        "    url = f\"{base_url}/search?q={search_query}\"\n",
        "\n",
        "    # 如果保存目錄不存在，則創建它\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    # 發送 GET 請求到網址\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # 檢查請求是否成功\n",
        "    if response.status_code == 200:\n",
        "        # 解析 HTML 內容\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # 找到所有圖片元素\n",
        "        image_elements = soup.find_all(\"img\")\n",
        "\n",
        "        # 下載的圖片計數器\n",
        "        downloaded_images = 0\n",
        "\n",
        "        # 遍歷圖片元素並下載圖片\n",
        "        for img in image_elements:\n",
        "            # 獲取圖片的源 URL\n",
        "            img_url = img.get(\"src\")\n",
        "\n",
        "            # 檢查圖片 URL 是否有效\n",
        "            if img_url:\n",
        "                # 下載圖片\n",
        "                img_response = requests.get(img_url)\n",
        "\n",
        "                # 檢查請求是否成功\n",
        "                if img_response.status_code == 200:\n",
        "                    # 將圖片保存到指定目錄\n",
        "                    with open(os.path.join(save_dir, f\"{category_name}_{downloaded_images}.jpg\"), \"wb\") as f:\n",
        "                        f.write(img_response.content)\n",
        "\n",
        "                    # 增加下載的圖片計數\n",
        "                    downloaded_images += 1\n",
        "\n",
        "                    # 檢查是否已下載所需數量的圖片\n",
        "                    if downloaded_images >= num_images:\n",
        "                        break\n",
        "\n",
        "        # 輸出一條消息，指示已下載的圖片數量\n",
        "        print(f\"已下載 {downloaded_images} 張圖片，類別：{category_name}\")\n",
        "\n",
        "# 定義主函數\n",
        "def main():\n",
        "    # 定義回收類別\n",
        "    categories = [\"紙箱\"]  # 根據需要添加更多類別\n",
        "\n",
        "    # 定義每個類別要爬取的圖片數量\n",
        "    num_images_per_category = 100\n",
        "\n",
        "    # 定義保存圖片的目錄\n",
        "    save_dir = \"回收圖片\"\n",
        "\n",
        "    # 遍歷每個類別並爬取圖片\n",
        "    for category in categories:\n",
        "        scrape_images_for_category(category, num_images_per_category, save_dir)\n",
        "\n",
        "# 呼叫主函數\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "dxPzR7UFmKjd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4rEK3WY1mOGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}